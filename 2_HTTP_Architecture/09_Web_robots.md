# HTTP 스터디 9장

  

## 09. 웹 로봇

  

9장의 흐름

  

> 웹 크롤러와 크롤링 과정에 대해 살펴본다.
> 이어서 크롤링 과정에서 조심해야 할 점과,
> 로봇의 요청과 응답 과정에서의 HTTP 규칙을 공부한다.
> 이어서 부적절한 웹 로봇은 무엇이며,
> 웹 로봇에서 적절한 접근 권한을 주는 방법을 살펴본다.
> 마지막으로 웹 로봇이 지켜야할 에티켓과
> 웹 로봇을 가장 광범위하게 사용하는 검색엔진을 공부한다.
<br/>
  

### 9.1 크롤러와 크롤링

  

>웹 로봇이란 사람과 상호작용 없이 연속된 웹 트렌젝션을 자동으로 수행하는 SW 프로그램으로, 
>웹 페이지들을 순회하는 로봇을 말한다.

  

1. 다음 그림의 웹을 크롤링하려고 한다. 다음 상황에서 가장 적합한 루트 집합을 만드시오.

  
![캡처](https://user-images.githubusercontent.com/76691680/105762999-879fda00-5f98-11eb-8477-4ded7c532e24.JPG)
  
<br/>
2. 일반적으로 좋은 루트 집합이란 _________과 ________________, 그리고 ______________으로 구성된다.

 <details>
	    <summary> 힌트</summary>
	    <div markdown="1">
	    
	    일단 시작을 위해서는 크고 인기 있는 ____가 있어야 한다.
	    
		(ex) http://www.yahoo.com과 같이.
		
		그리고 응애응애하는 페이지와,

		그리고 외로워하는 페이지도 포함해 주어야 할 것이다.
	    
</details>

<br/><br/>
3. 다음은 순환에 대한 설명이다. 옳지 않은 것을 고르시오.

  

(1) 로봇이 웹을 크롤링할 때, 순환에 빠지지 않도록 조심해야 한다.

  

(2) 순환에 빠진 크롤러는 같은 페이지들을 반복해서 가져오는데 시간을 허비할 수 있다.

  

(3) 순환에 빠진 크롤러는 웹 사이트에 과부하를 유발해 사용자의 접근도 방해할 수 있다.

  

(4) 순환은 쓸모없는 중복된 컨텐츠들로 스스로를 낭비하게 만든다.

  
<br/><br/>

> 이러한 이유로 순환은 크롤러에게 해롭다.
> 따라서 크롤러는 자신이 어디를 방문했는지 알아야 한다.
> URL은 굉장히 많기 때문에 방문 여부를 판단하기 위해서는
> 복잡한 자료구조를 사용할 필요가 있다.



4. 다음은 웹 크롤러가 자신이 방문한 곳을 관리하기 위해 사용하는 기법들이다. 옳지 않은 것을 고르시오.

  

	(1) 검색 트리

  

	(2) 해시 테이블

  

	(3) 느슨한 존재 비트맵

  

	(4) 체크섬

  

	(5) 파티셔닝

  
<br/>

5. 다음은 로봇이 웹에서 루프와 중복을 피하기 위해 사용하는 기법에 대한 설명이다. 보기를 사용해 빈칸을 적절히 채우시오.

  

```markdown

보기 : 스로틀링, URL 크기제한, URL 정규화, 콘텐츠 지문, URL 블랙리스트,

너비 우선 크롤링, 패턴 발견

```

  

(1) ____________ : 같은 리소스를 가리키는 중복된 URL이 생기는 것을 방지한다.

  

(2) ____________ : 혹시 함정에 빠지더라도, 옆 웹사이트들에서 정보를 받아올 수 있다.

  

(3) ____________ : 로봇이 웹사이트에서 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한한다.

  

(4) ____________ : 순환으로 인해 URL이 계속해서 길어질 경우 순환을 중단시킨다.

  

(5) ____________ : 순환을 유발하거나 함정으로 판단되어 기피해야할 목록을 말한다.

  

(6) ____________ : 심벌릭 링크에 의한 순환과 같이, 반복되는 구성요소를 가진 URL을 의심한다.

  

(7) ____________ : 페이지의 콘텐츠에서 얻어낸 체크섬을 비교해 중복을 막는다.
<br/><br/>
  
 <details>
	    <summary> 답지</summary>
	    <div markdown="1">
	    
		1. A,S,G ;웹의 모든 것을 커버하기 위해서는 다음 셋만 루트집합에 있으면 된다.
		
		2. (크고 인기있는)웹사이트, 새로 생성된 페이지 목록, 자주 링크되지 않는 페이지들 목록

		3. 모두 옳은 선지다.

		4. (4) ;체크섬은 방문했었는지 판단하는데 사용되는 것으로, 방문한 곳을 관리하기 위해서는 체크 포인트가 사용된다.

		5. (1) URL 정규화, (2) 너비 우선 크롤링, (3) 스로틀링, (4) URL 크기 제한, 
		   (5) URL 블랙리스트, (6) 패턴 발견, (7) 콘텐츠 지문  
</details>
<br/><br/>
  

### 9.2 로봇의 HTTP

  

>로봇도 다른 HTTP 프로그램과 다르지 않아서, HTTP 명세의 규칙을 지켜야 한다.
>요청에는 User-Agent, From , Accept 헤더를 포함해 신원을 알려주는 기본 헤더들을 사이트에 보내주는 것이 좋다.

  <br/>

6. 다음은 크롤러가 두 개의 사이트를 운영하는 서버에 Host 헤더 없이 요청을 보냈을 때의 상황이다.

  

서버가 `www.joes-hardware.com`과 [`www.foo.com`](http://www.foo.com) 두 개의 사이트를 운영한다고 할 때, 다음 상황에서 발생하는 문제를 서술하시오.

  

![캡처2](https://user-images.githubusercontent.com/76691680/105763002-88387080-5f98-11eb-9748-f21b573f1478.JPG)

6-1) 그렇다면 Host 헤더를 포함했을 때는?
<br/><br/>
  
 <details>
	    <summary> 답지</summary>
	    <div markdown="1">
	    
	    6. 위의 그림에서 서버는 `www.joes-hardware.com`를 기본으로 제공한다.

		 따라서 이와 같은 상황에서는, 크롤러는 foo.com의 /index.html을 요구했지만 
		 Host 헤더를 포함하지 않았기 때문에 `www.joes-hardware.com`에 대한 콘텐츠를 
		 받게 되는 문제가 생긴다.
		 또한 이렇게 받은 `www.joes-hardware.com`에 대한 콘텐츠를 
		 크롤러는 foo.com에서 온 것이라고 생각한다는 문제가 있다.


		6-1. Host 헤더를 포함하면 제대로 찾아가겠죠.

	    
</details>
<br/><br/>
  

### 9.3 부적절하게 동작하는 로봇들

  

>제멋대로인 로봇의 예
>- 에러/순환에 빠진 로봇 → 서버에 과부하
>- 오래된 URL → 존재하지 않는 문서에 대한 접근 요청
>- 길고 잘못된 URL → 웹 서버의 처리 능력에 영향
>- 사적 데이터까지 접근하는 로봇 → 그럼 못쓰지
>- 동적 게이트웨이 접근 → 처리 비용 多

  <br/><br/>

### 9.4 로봇 차단하기

  

7. 다음은 로봇의 동작 및 접근을 제어할 수 있는 robots.txt에 대한 단계별 설명이다. 틀린 부분을 찾아 고치시오.

  

(1) robots.txt에는 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보가 담겨있다.

(2) 따라서 로봇은 페이지를 요청하기 전에 해당 페이지에 접근 권한이 있는지 확인하기 위해 robots.txt 파일을 검사할 필요가 있다.

(3) 대부분의 서버가 robots.txt를 제공하지 않지만 로봇은 가장 먼저 robots.txt부터 확인한다.

  

(4) GET 메서드를 이용해 robots.txt 리소스를 가져온다.

  

(5) 로봇의 요청에 대한 서버의 응답으로 404 상태코드가 온다면, 해당 로봇은 접근 권한이 없다고 해석한다.

  

(6) 서버의 응답으로 2xx 상태코드가 온다면, 로봇은 반드시 응답의 콘텐츠를 파싱하여 차단 규칙을 얻고 해당 여부를 판단한다.

  

(7) 로봇은 매 파일 접근마다 robots.txt를 새로 가져오는 것이 아니라, 주기적으로 robots.txt를 가져와서 그 결과를 캐시해야 한다.

  
<br/><br/>
8. 다음 표는 robots.txt 파일 포맷 중 Disallow와 Allow 줄의 접두매칭의 예를 나타낸 것이다. 빈칸에 알맞게 등호(=)와 부등호(≠)를 표시하시오. (단, 이스케이핑 된 문자 %7E와 ~는 같고, %2F와 /은 같다)

  

| 규칙 경로 | URL 경로 | 매치되는가? |
|-------|-------|------|
|/tmp|/tmp|=|
|/tmp|/tmpfile.html|=|
|/tmp/|/tmp|(1)|
||README.TXT|=|
|/~fred/hi.html|/%7Efred/hi.html|(2)|
|/~fred/hi.html|/~fred%2Fhi.html|(3)|

  
<br/><br/>
9. 다음의 문장에서 `A`, `B`, `C`에 들어갈 말을 채우시오.

  

```markdown

robots.txt 파일의 단점 중 하나는 robots.txt를 콘텐츠 A가 아니라

웹 사이트 B가 소유한다는 것이다. A 는 개별 페이지에 대한 로봇의 접근을

좀 더 직접 제한하기 위해 HTML 문서에 C 를 추가할 수 있다.

```
<br/>
  
<details>
	    <summary> 답지</summary>
	    <div markdown="1">
	    
		7. (5) 
	      ;서버가 404를 응답하면 로봇은 활성화된 차단 규칙이 존재하지 않는다고 가정하고 
	      robots.txt의 제약 없이 사이트에 접근할 수 있다.

  

		8. (1) ≠, (2) =, (3) ≠
		  ;접두 매칭이므로 URL 경로의 시작부터 규칙 경로의 길이만큼 서로 같아야 한다.
		   또한 이스케이핑된 문자들은 원래대로 복원되어 비교되지만, 
		   빗금을 의미하는 %2F만 예외로 둘다 /로 같아야만 같은 것으로 인정한다.


		9.  A : 작성자(저자)
		    B : 관리자
		    C : 로봇 제어 META 태그

	    
</details>

<br/><br/>
  
  

### 9.6 검색엔진

  

10. 다음 그림에서 (A)와 (B)를 채우시오.

  

![캡처3](https://user-images.githubusercontent.com/76691680/105763004-88d10700-5f98-11eb-996f-1669dc6ddd38.JPG)
  

10-1. 위와 같은 상용 검색엔진에서, 사용자가 검색어를 입력했을 때 검색 결과를 보여주는 과정을 설명하시오.

  <br/><br/>
<details>
	    <summary> 답지</summary>
	    <div markdown="1">
	    
		10. (A) : 풀 텍스트 색인 데이터베이스
			(B) : 검색엔진 크롤러/로봇

  

		10-1.
		✅ 사용자가 검색어를 입력한다.
		
		→ 하드웨어 웹 서버는 사용자로부터 HTTP GET/POST 등의 요청이 들어오면
		이 질의를 검색 게이트웨이에 넘겨준다.

		→ 게이트웨이는 풀 텍스트 색인 데이터베이스를 이용해
		입력 검색어를 포함한 문서들을 즉각 알려준다.
		(크롤러가 웹 서버들 돌아다니면서 준 정보들을 바탕으로 구성해놓은 것)

		→ 웹 검색 게이트웨이는 웹 서버에게 검색 결과 문서 목록을 
		결과로 돌려준다.

		→ 웹 서버는 이 결과를 사용자를 위한 HTML 페이지로 변환해 돌려준다.
	    
</details>

  


